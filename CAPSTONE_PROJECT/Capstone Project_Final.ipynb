{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tourist Visitor Information Database for US Cities\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "US Federal body for Toursim Management(USATM) has hired my organisation Rusty's Analtics & Data Consulting(RADC) to extract, transform & load immigration and cities data to one table for their team to carry out advanced analytics. USATM has an advanced analytics team that would like to run use cases such as identifying cities with most tourist footfall and then accordingly allocate budgets for marketing/advertising spend for the upcoming tourist season. RADC is an expert in this area and would like to help them out with our expertise in the Data & Analytics area.  \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "We will be reading 2 files - immigration_data_sample.csv and us-cities-demograpgics.csv from S3. Once we have read the 2 files, we will perform Exploratory Data Analysis to review the data and file contents(using the describe function for review). We will then create fact & dimensions table from the data and perform data quality checks once the files are loaded in Redshift. These tables will be available in Redshift for the USATM team to utilise for further insights.   \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "I will be using the udacity provided data sets - immigration_data_sample.csv & us-cities-demographics.csv. Immigration data set contains the list of all inflow to the USA via different modes of transport for a certain period. Other CSV with US cities demographics has resident demographic information for each city in the USA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.000000000</td>\n",
       "      <td>2016.000000000</td>\n",
       "      <td>4.000000000</td>\n",
       "      <td>209.000000000</td>\n",
       "      <td>209.000000000</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.000000000</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.000000000</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>nan</td>\n",
       "      <td>JL</td>\n",
       "      <td>56582674633.000000000</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.000000000</td>\n",
       "      <td>2016.000000000</td>\n",
       "      <td>4.000000000</td>\n",
       "      <td>582.000000000</td>\n",
       "      <td>582.000000000</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.000000000</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.000000000</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>nan</td>\n",
       "      <td>*GA</td>\n",
       "      <td>94361995930.000000000</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.000000000</td>\n",
       "      <td>2016.000000000</td>\n",
       "      <td>4.000000000</td>\n",
       "      <td>148.000000000</td>\n",
       "      <td>112.000000000</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.000000000</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.000000000</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>nan</td>\n",
       "      <td>LH</td>\n",
       "      <td>55780468433.000000000</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.000000000</td>\n",
       "      <td>2016.000000000</td>\n",
       "      <td>4.000000000</td>\n",
       "      <td>297.000000000</td>\n",
       "      <td>297.000000000</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.000000000</td>\n",
       "      <td>1.000000000</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.000000000</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>nan</td>\n",
       "      <td>QR</td>\n",
       "      <td>94789696030.000000000</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.000000000</td>\n",
       "      <td>2016.000000000</td>\n",
       "      <td>4.000000000</td>\n",
       "      <td>111.000000000</td>\n",
       "      <td>111.000000000</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.000000000</td>\n",
       "      <td>3.000000000</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.000000000</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42322572633.000000000</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             cicid          i94yr      i94mon        i94cit  \\\n",
       "0     2027561 4084316.000000000 2016.000000000 4.000000000 209.000000000   \n",
       "1     2171295 4422636.000000000 2016.000000000 4.000000000 582.000000000   \n",
       "2      589494 1195600.000000000 2016.000000000 4.000000000 148.000000000   \n",
       "3     2631158 5291768.000000000 2016.000000000 4.000000000 297.000000000   \n",
       "4     3032257  985523.000000000 2016.000000000 4.000000000 111.000000000   \n",
       "\n",
       "         i94res i94port         arrdate     i94mode i94addr    ...     \\\n",
       "0 209.000000000     HHW 20566.000000000 1.000000000      HI    ...      \n",
       "1 582.000000000     MCA 20567.000000000 1.000000000      TX    ...      \n",
       "2 112.000000000     OGG 20551.000000000 1.000000000      FL    ...      \n",
       "3 297.000000000     LOS 20572.000000000 1.000000000      CA    ...      \n",
       "4 111.000000000     CHM 20550.000000000 3.000000000      NY    ...      \n",
       "\n",
       "   entdepu  matflag        biryear   dtaddto  gender insnum airline  \\\n",
       "0      nan        M 1955.000000000  07202016       F    nan      JL   \n",
       "1      nan        M 1990.000000000  10222016       M    nan     *GA   \n",
       "2      nan        M 1940.000000000  07052016       M    nan      LH   \n",
       "3      nan        M 1991.000000000  10272016       M    nan      QR   \n",
       "4      nan        M 1997.000000000  07042016       F    nan     NaN   \n",
       "\n",
       "                 admnum  fltno  visatype  \n",
       "0 56582674633.000000000  00782        WT  \n",
       "1 94361995930.000000000  XBLNG        B2  \n",
       "2 55780468433.000000000  00464        WT  \n",
       "3 94789696030.000000000  00739        B2  \n",
       "4 42322572633.000000000   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here from S3 bucket\n",
    "fname = 'immigration_data_sample.csv'\n",
    "df = pd.read_csv(fname)\n",
    "\n",
    "#Use Describe function to check basics of the data set. \n",
    "#Initial EDA Reveals: \n",
    "#Records are from Year 2016\n",
    "#1000 records in the file\n",
    "#Data is for the month of April\n",
    "#75% arrivals are by Air\n",
    "#Majority of passengers are travelling for 'Pleasure' \n",
    "#Average age of travellers is 43, Oldest passenger is 93, Youngest is 1 year old.\n",
    "#df.describe()\n",
    "print(\"First 5 rows\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print information, shape, and data type for the data frame\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 29 columns):\n",
      "Unnamed: 0    1000 non-null int64\n",
      "cicid         1000 non-null float64\n",
      "i94yr         1000 non-null float64\n",
      "i94mon        1000 non-null float64\n",
      "i94cit        1000 non-null float64\n",
      "i94res        1000 non-null float64\n",
      "i94port       1000 non-null object\n",
      "arrdate       1000 non-null float64\n",
      "i94mode       1000 non-null float64\n",
      "i94addr       941 non-null object\n",
      "depdate       951 non-null float64\n",
      "i94bir        1000 non-null float64\n",
      "i94visa       1000 non-null float64\n",
      "count         1000 non-null float64\n",
      "dtadfile      1000 non-null int64\n",
      "visapost      382 non-null object\n",
      "occup         4 non-null object\n",
      "entdepa       1000 non-null object\n",
      "entdepd       954 non-null object\n",
      "entdepu       0 non-null float64\n",
      "matflag       954 non-null object\n",
      "biryear       1000 non-null float64\n",
      "dtaddto       1000 non-null object\n",
      "gender        859 non-null object\n",
      "insnum        35 non-null float64\n",
      "airline       967 non-null object\n",
      "admnum        1000 non-null float64\n",
      "fltno         992 non-null object\n",
      "visatype      1000 non-null object\n",
      "dtypes: float64(15), int64(2), object(12)\n",
      "memory usage: 226.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Print information, shape, and data type for the data frame\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.800000000</td>\n",
       "      <td>40601.000000000</td>\n",
       "      <td>41862.000000000</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.000000000</td>\n",
       "      <td>30908.000000000</td>\n",
       "      <td>2.600000000</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.000000000</td>\n",
       "      <td>44129.000000000</td>\n",
       "      <td>49500.000000000</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.000000000</td>\n",
       "      <td>32935.000000000</td>\n",
       "      <td>2.390000000</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.500000000</td>\n",
       "      <td>38040.000000000</td>\n",
       "      <td>46799.000000000</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.000000000</td>\n",
       "      <td>8229.000000000</td>\n",
       "      <td>2.580000000</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.500000000</td>\n",
       "      <td>88127.000000000</td>\n",
       "      <td>87105.000000000</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.000000000</td>\n",
       "      <td>33878.000000000</td>\n",
       "      <td>3.180000000</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.600000000</td>\n",
       "      <td>138040.000000000</td>\n",
       "      <td>143873.000000000</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.000000000</td>\n",
       "      <td>86253.000000000</td>\n",
       "      <td>2.730000000</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State   Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland 33.800000000  40601.000000000   \n",
       "1            Quincy  Massachusetts 41.000000000  44129.000000000   \n",
       "2            Hoover        Alabama 38.500000000  38040.000000000   \n",
       "3  Rancho Cucamonga     California 34.500000000  88127.000000000   \n",
       "4            Newark     New Jersey 34.600000000 138040.000000000   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans    Foreign-born  \\\n",
       "0    41862.000000000             82463      1562.000000000 30908.000000000   \n",
       "1    49500.000000000             93629      4147.000000000 32935.000000000   \n",
       "2    46799.000000000             84839      4819.000000000  8229.000000000   \n",
       "3    87105.000000000            175232      5821.000000000 33878.000000000   \n",
       "4   143873.000000000            281913      5829.000000000 86253.000000000   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0             2.600000000         MD         Hispanic or Latino  25924  \n",
       "1             2.390000000         MA                      White  58723  \n",
       "2             2.580000000         AL                      Asian   4759  \n",
       "3             3.180000000         CA  Black or African-American  24437  \n",
       "4             2.730000000         NJ                      White  76402  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read file from a S3 Bucket\n",
    "fname = 'us-cities-demographics.csv'\n",
    "df1 = pd.read_csv(fname, delimiter=';')\n",
    "\n",
    "#Use this function to convert data with scientific notation to float\n",
    "pd.set_option('display.float_format', lambda x: '%.9f' % x)\n",
    "\n",
    "#Use Describe function to check basics of the data set but first.\n",
    "#Initial EDA Reveals: \n",
    "#Total records is 2891\n",
    "#Average age of regidents in the file is 35\n",
    "#Max male population is 4.08 million\n",
    "#Max female population is 4.5 million\n",
    "#Total population is 8.56 million\n",
    "#On average there are 9370 veterans across US cities\n",
    "#Across cities in the US - 40,653 residents are foreign-born\n",
    "#df1.describe()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print information, shape, and data type for the data frame\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Print information, shape, and data type for the data frame\")\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "cicid            0\n",
       "i94yr            0\n",
       "i94mon           0\n",
       "i94cit           0\n",
       "i94res           0\n",
       "i94port          0\n",
       "arrdate          0\n",
       "i94mode          0\n",
       "i94addr         59\n",
       "depdate         49\n",
       "i94bir           0\n",
       "i94visa          0\n",
       "count            0\n",
       "dtadfile         0\n",
       "visapost       618\n",
       "occup          996\n",
       "entdepa          0\n",
       "entdepd         46\n",
       "entdepu       1000\n",
       "matflag         46\n",
       "biryear          0\n",
       "dtaddto          0\n",
       "gender         141\n",
       "insnum         965\n",
       "airline         33\n",
       "admnum           0\n",
       "fltno            8\n",
       "visatype         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count unique values in immigration dataframe\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "cicid            0\n",
       "i94yr            0\n",
       "i94mon           0\n",
       "i94cit           0\n",
       "i94res           0\n",
       "i94port          0\n",
       "arrdate          0\n",
       "i94mode          0\n",
       "i94addr         59\n",
       "depdate         49\n",
       "i94bir           0\n",
       "i94visa          0\n",
       "count            0\n",
       "dtadfile         0\n",
       "visapost       618\n",
       "occup          996\n",
       "entdepa          0\n",
       "entdepd         46\n",
       "entdepu       1000\n",
       "matflag         46\n",
       "biryear          0\n",
       "dtaddto          0\n",
       "gender         141\n",
       "insnum         965\n",
       "airline         33\n",
       "admnum           0\n",
       "fltno            8\n",
       "visatype         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if any rows in the immigration data frame have missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "cicid            0\n",
       "i94yr            0\n",
       "i94mon           0\n",
       "i94cit           0\n",
       "i94res           0\n",
       "i94port          0\n",
       "arrdate          0\n",
       "i94mode          0\n",
       "i94addr          0\n",
       "depdate          0\n",
       "i94bir           0\n",
       "i94visa          0\n",
       "count            0\n",
       "dtadfile         0\n",
       "visapost         0\n",
       "occup            0\n",
       "entdepa          0\n",
       "entdepd         46\n",
       "entdepu       1000\n",
       "matflag         46\n",
       "biryear          0\n",
       "dtaddto          0\n",
       "gender           0\n",
       "insnum         965\n",
       "airline          0\n",
       "admnum           0\n",
       "fltno            8\n",
       "visatype         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill NaN with a blank space\n",
    "df['i94addr'] = df['i94addr'].fillna(' ')\n",
    "df['depdate'] = df['depdate'].fillna(' ')\n",
    "df['visapost'] = df['visapost'].fillna(' ')\n",
    "df['occup'] = df['occup'].fillna(' ')\n",
    "df['gender'] = df['gender'].fillna(' ')\n",
    "df['airline'] = df['airline'].fillna(' ')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      False\n",
      "1      False\n",
      "2      False\n",
      "3      False\n",
      "4      False\n",
      "5      False\n",
      "6      False\n",
      "7      False\n",
      "8      False\n",
      "9      False\n",
      "10     False\n",
      "11     False\n",
      "12     False\n",
      "13     False\n",
      "14     False\n",
      "15     False\n",
      "16     False\n",
      "17     False\n",
      "18     False\n",
      "19     False\n",
      "20     False\n",
      "21     False\n",
      "22     False\n",
      "23     False\n",
      "24     False\n",
      "25     False\n",
      "26     False\n",
      "27     False\n",
      "28     False\n",
      "29     False\n",
      "       ...  \n",
      "970    False\n",
      "971    False\n",
      "972    False\n",
      "973    False\n",
      "974    False\n",
      "975    False\n",
      "976    False\n",
      "977    False\n",
      "978    False\n",
      "979    False\n",
      "980    False\n",
      "981    False\n",
      "982    False\n",
      "983    False\n",
      "984    False\n",
      "985    False\n",
      "986    False\n",
      "987    False\n",
      "988    False\n",
      "989    False\n",
      "990    False\n",
      "991    False\n",
      "992    False\n",
      "993    False\n",
      "994    False\n",
      "995    False\n",
      "996    False\n",
      "997    False\n",
      "998    False\n",
      "999    False\n",
      "Length: 1000, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicated data\n",
    "bool_series = df.duplicated()\n",
    "print(bool_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       567\n",
       "State                       49\n",
       "Median Age                 180\n",
       "Male Population            593\n",
       "Female Population          594\n",
       "Total Population           594\n",
       "Number of Veterans         577\n",
       "Foreign-born               587\n",
       "Average Household Size     161\n",
       "State Code                  49\n",
       "Race                         5\n",
       "Count                     2785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count unique values in US Cities dataframe\n",
    "df1.nunique()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if any rows in the us cities demographics data frame have missing data\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                      0\n",
       "State                     0\n",
       "Median Age                0\n",
       "Male Population           0\n",
       "Female Population         0\n",
       "Total Population          0\n",
       "Number of Veterans        0\n",
       "Foreign-born              0\n",
       "Average Household Size    0\n",
       "State Code                0\n",
       "Race                      0\n",
       "Count                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill NaN with a mean value\n",
    "df1['Male Population'] = df1['Male Population'].fillna(df1['Male Population'].mean())\n",
    "df1['Female Population'] = df1['Female Population'].fillna(df1['Female Population'].mean())\n",
    "df1['Number of Veterans'] = df1['Number of Veterans'].fillna(df1['Number of Veterans'].mean())\n",
    "df1['Average Household Size'] = df1['Average Household Size'].fillna(df1['Average Household Size'].mean())\n",
    "df1['Foreign-born'] = df1['Foreign-born'].fillna(df1['Foreign-born'].mean())\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "Created 3 tables - Immigration, Arrival & State. Please refer to the attachment in my project folder to get a diagrammatic view. I used this model as it would help USATM answer queries such as states with most tourism influx in a year, \n",
    "arrival data table would provide insights on days that are particularly popular with tourists and the state table can be used to see the population of a state over a given year and can be expanded\n",
    "to provide a view on annual basis to assist government agencies to prepare better for immigration inflow.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "1. create_table.py is where I will create my fact and dimension tables for the star schema in Redshift.\n",
    "2. etl.py is where I will load data from S3 into staging tables on Redshift and then process that data into my analytics tables on Redshift.\n",
    "3. sql_queries.py is where I have defined my SQL statements, which will be imported into the two other files above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#1. Run sql_queries.py in the terminal\n",
    "#2. Run create_tables.py in the terminal\n",
    "#3. Run ETL.py in the terminal\n",
    "# Once the above scripts are run, data from S3 buckets would be loaded into Redshift tables for analysis. \n",
    "#4. We now have 3 tables(Immigration, State, Arrival) that are ready for further analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Perform quality checks here\n",
    "#1. Integrity constraints have been included in the script sql_queries.py under the CREATE table statement. We have used NOT NULL, PRIMARY KEY constraints while creation.\n",
    "#2. All scripts have been tested and run without any errors. Please refer to the screenshots attached in github.\n",
    "#3. Source/Count checks on immigration table have revealed 1000 entries in the CSV, 1000 in staging_immi table and 1000 in the final immi table. We are all good. Screenshots captured in github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "1. Rationale for choice of tools and technologies for the project. \n",
    "\n",
    "Q) Why did you choose specific tools & tech?\n",
    "Answer) We have chosen Amazon suite of products - S3, EC2, Redshift, Jupyter Notebook, Airflow, Python 3. We have used the cloud technologies mainly to deal with the\n",
    "growing nature of big data and also it allows us to minimise costs by shutting down or going in sleep modes for periods of low demand. Airflow is best of breed\n",
    "and open source tech. It is widely used in the industry for data orchestration and provides a reliable way to schedule the pipeline as per business requirements. Airflow\n",
    "also has an active user(developer) community and roll out new features/enhacements regularly.  \n",
    "\n",
    "Q) Why the chosen schema?\n",
    "Answer) We have 2 staging tables(staging_immi and staging_state) & 3 transformed tables(immigration, arrival, state). Landing Zone: This has the staging tables and will capture all \n",
    "data from source in as-is format and it will have truncate and load strategy. From staging area - data is loaded to the Transform Zone and will have the 3 tables mentioned above.\n",
    "\n",
    "Q) Is there a trade-off?\n",
    "Answer) The only trade-off that we can think about is the huge amount of data and the costs associated with that. As the amount of data goes up, it will start to blow up our cloud resources\n",
    "costs. \n",
    "\n",
    "Q) Furnish results of the sample queries mentioned in scope.\n",
    "Answer) Please refer to the github screenshots(Query 1, Query 2) of 2 queries that were executed on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. How often should the data be updated and why?\n",
    "\n",
    "From our engagement with USATM, we found that this information is required on a quarterly basis to ensure\n",
    "right and fair allocation of funding to boost state specific tourism in the United States. It is hence desired\n",
    "to update the data every 2nd month of a quarter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "3. Write a description of how you would approach the problem differently under the following scenarios:\n",
    "Q) The data was increased by 100x. \n",
    "- Is partitioning required? If so, how\n",
    "Answer) I have 3 tables that we are using - immigration, arrival and state. We don't expect partition for state as it is a master table and is not expected\n",
    "to grow drastically. (Please refer to data model on Github)Let's discuss immigration and arrival - we will partition these two tables based on arrival year and arrival month.  \n",
    "\n",
    "- How can storage be handeled?\n",
    "Answer) Since we are using arrival year and arrival month as partition columns, this will help further in applying the purging policy and free up storage. We can also\n",
    "archive the cold data to another storage like aws glacier.\n",
    "\n",
    "\n",
    "Q) The data populates a dashboard that must be updated on a daily basis by 7am every day. \n",
    "- What are the configuration you need to do in identified tool?\n",
    "Answer) We will be looking for an orchestrating tool(Airflow), which will orchestrate our ETL pipeline and make data available in datawarehouse before 7am. We will schedule the Airflow by updating the CRON entry \"0 7 * * *\".\n",
    "\n",
    "- How will this impact business ?\n",
    "Answer) Airflow is an open source tool so we don't have to pay a license fee. From a business point, there should be minimal impact as we will be putting in alerts\n",
    "and quality checks(such as Airflow run time & defined SLAs so that critical jobs can finish before 7 AM). \n",
    "\n",
    "- What is the operating cost ?\n",
    "Answer) We intend to operate a medium Managed Workflows environment with Apache Airflow version 2.0.2 in the US East (N. Virginia) region where our variable demand \n",
    "requires 10 workers simultaneously for 2 hours a day, we require a total of 3 schedulers to manage our workflow definitions, and retain 40 GB of data \n",
    "(approximately 200 daily workflows, each with 20 tasks, stored for 6 months), we would pay the following for the month:\n",
    "\n",
    "Environment charge\n",
    "Instance usage (in hours) = 31 days x 24 hrs/day = 744 hours\n",
    "x $0.74 (price per hour for a medium environment in the US East (N. Virginia) region)\n",
    "= $ 550.56\n",
    "\n",
    "Worker charge\n",
    "Instance usage (in hours) = 31 days x 2 hrs/day x 9 additional instances (10 less 1 included with environment) = 558 hours\n",
    "x $0.11 (price per hour for a medium worker in the US East (N. Virginia) region)\n",
    "= $61.38\n",
    "\n",
    "Scheduler charge\n",
    "Instance usage (in hours) = 31 days x 24 hrs/day x 1 additional instances (3 less 2 included with environment) = 774 hours\n",
    "x $0.11 (price per hour for a medium scheduler in the US East (N. Virginia) region)\n",
    "= $81.84\n",
    "\n",
    "Meta database charge\n",
    "40 GB or storage x $0.10 GB-month = $4.00\n",
    "Total charge = $697.78\n",
    "\n",
    "- Does the tool require a separate instance ?\n",
    "Answer) We will spin up a new EC2 instance and our Airflow will be hosted on that VM. We can also reduce cost using Lambda function to build infrstructure on demand. This means that whenever task is completed, we will shutdown our infra to save on costs. \n",
    "\n",
    "Q) The database needed to be accessed by 100+ people.\n",
    "- Does data replication make sense? \n",
    "Answer) Considering all of our user base is based out of United States, we will not be proposing a data replication strategy for the short terms. However, if we find\n",
    "that the data has grown immensely and there is heavy latency then data replication will need to be brought to life. Please note that data replication comes at a cost\n",
    "and this decision will be based on cost versus benefit analysis.\n",
    "\n",
    "- What type of replication ?\n",
    "Answer) We will be opting for log-based incremental replication. Log-based incremental replication is a special case of replication that applies only to database \n",
    "sources. This process replicates data based on information from the database log file, which lists changes to the database. This method is the most efficient out of  \n",
    "the three(Full table replication, Key-based incremental replication), but it must be supported by the source database, as it is by MySQL, PostgreSQL, and Oracle. This \n",
    "method works best if the source database structure is relatively static(which is the case for our project).\n",
    "\n",
    "- Business benefits and impacts on cost?\n",
    "Answer) Some benefits of data replication are:\n",
    "a) Improved reliability and availability: If one system goes down due to faulty hardware, malware attack, or another problem, the data can be accessed from a \n",
    "    different site.\n",
    "\n",
    "b) Improved network performance: Having the same data in multiple locations can lower data access latency, since required data can be retrieved closer to \n",
    "    where the transaction is executing.\n",
    "\n",
    "c) Increased data analytics support: Replicating data to a data warehouse empowers distributed analytics teams to work on common projects for business intelligence.\n",
    "\n",
    "d) Improved test system performance: Data replication facilitates the distribution and synchronization of data for test systems that demand fast data accessibility.\n",
    "\n",
    "Cost Impacts: Some of the challenges to maintaining consistent data across an organization boil down to limited resources:\n",
    "a) Money: Keeping copies of the same data in multiple locations leads to higher storage and processor costs.\n",
    "\n",
    "b) Time: Implementing and managing a data replication system requires dedicated time from an internal team.\n",
    "\n",
    "c) Bandwidth: Maintaining consistency across data copies requires new procedures and adds traffic to the network.\n",
    "    \n",
    "We would need to perform a cost-benefit analysis of the tool the replication strategy and then roll forward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
